{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b95fa393-242f-4ecb-ac82-94915c19335c",
   "metadata": {},
   "source": [
    "# AssQ 16-Apr Boosting-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae03804b-51bc-424f-a8d0-c26e7aceecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c682d4-2ed7-4c57-b163-b6141d7f1ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting is a method used in machine learning to reduce errors in predictive data analysis.\n",
    "Data scientists train machine learning software, called machine learning models,\n",
    "on labeled data to make guesses about unlabeled data.\n",
    "\n",
    "Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. \n",
    "In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is,\n",
    "each model tries to compensate for the weaknesses of its predecessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff296005-02a9-42bd-8161-a9f44bcdd3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17d09c-b35d-40c3-8720-f3edacb9058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a4c9ff-275c-4481-acc2-68196329afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "The key benefits of boosting include:\n",
    "Ease of Implementation: Boosting can be used with several hyper-parameter tuning options to improve fitting. ...\n",
    "Reduction of bias: Boosting algorithms combine multiple weak learners in a sequential method, iteratively improving upon observations.\n",
    "\n",
    "Boosting decreases bias, not variance. In Bagging, each model receives an equal weight.\n",
    "In Boosting, models are weighed based on their performance.\n",
    "\n",
    "\n",
    "Boosting is a resilient method that curbs over-fitting easily. \n",
    "One disadvantage of boosting is that it is sensitive to outliers since every\n",
    "classifier is obliged to fix the errors in the predecessors. \n",
    "Thus, the method is too dependent on outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a3ab0-9729-4c28-96ff-74b4ea584499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba73ca-7769-4ab4-bc74-0c4367e63815",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85890e51-2118-4cf7-8ada-d79d4e1b30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. \n",
    "In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is,\n",
    "each model tries to compensate for the weaknesses of its predecessor.\n",
    "\n",
    "Boosting creates an ensemble model by combining several weak decision trees sequentially.\n",
    "It assigns weights to the output of individual trees. \n",
    "Then it gives incorrect classifications from the first decision tree a higher weight and input to the next tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b020a-b38a-4997-b010-8f7b1d317566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd813d4-a7f3-4c37-b3b4-afcce23553bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6238f-0b3f-4e14-9248-0d48f265bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are three types of Boosting Algorithms which are as follows:\n",
    "    \n",
    "AdaBoost (Adaptive Boosting) algorithm.\n",
    "Gradient Boosting algorithm.\n",
    "XG Boost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f38367f-af6d-4624-a2a7-ffdae0773cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e5934-c77c-4de1-a9a0-3c78bdc80aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dae5b9-4726-4465-8ad0-1d0e264077c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are many parameters, but below are a few key defaults.\n",
    "learning_rate=0.1 (shrinkage).\n",
    "n_estimators=100 (number of trees).\n",
    "max_depth=3.\n",
    "min_samples_split=2.\n",
    "min_samples_leaf=1.\n",
    "subsample=1.0.\n",
    "\n",
    "\n",
    "\n",
    "GBM (Boosted Models) Tuning Parameters\n",
    "n. trees – Number of trees (the number of gradient boosting iteration) i.e. N. ...\n",
    "interaction. depth (Maximum nodes per tree) - number of splits it has to perform on a tree (starting from a single node). ...\n",
    "Shrinkage (Learning Rate) – It is considered as a learning rate. ...\n",
    "n. ...\n",
    "bag. ...\n",
    "train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6704cde9-f6a0-4379-8302-d152b9dd4c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843012d-40a9-499d-a064-b1204d16dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbea0e5a-e611-4076-9b68-b3862c316489",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting is an ensemble learning method that combines a set of weak learners \n",
    "into a strong learner to minimize training errors. In boosting, a random sample of data is selected,\n",
    "fitted with a model and then trained sequentially—that is, \n",
    "each model tries to compensate for the weaknesses of its predecessor.\n",
    "\n",
    "\n",
    "Boosting is an algorithm that helps in reducing variance and bias in a machine learning ensemble.\n",
    "The algorithm helps in the conversion of weak learners into strong learners by combining N number of learners.\n",
    "Boosting also can improve model predictions for learning algorithms\n",
    "\n",
    "The boosting algorithm assigns equal weight to each data sample. \n",
    "It feeds the data to the first machine model, called the base algorithm. \n",
    "The base algorithm makes predictions for each data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ea71a-4cf0-4f05-a836-a54acfd32d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc3247b-a2e8-418c-a357-974a6125954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc59706-5a5f-4c1e-9493-1298a3a006d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost, also called Adaptive Boosting, is a technique in Machine Learning used as an Ensemble Method.\n",
    "The most common estimator used with AdaBoost is decision trees with one level which means Decision trees with only 1 split. \n",
    "These trees are also called Decision Stumps.\n",
    "\n",
    "It is called Adaptive Boosting as the weights are re-assigned to each instance,\n",
    "with higher weights assigned to incorrectly classified instances.\n",
    "\n",
    "AdaBoost is used to cascade multiple weak classifiers to form a strong classifier, \n",
    "thus achieving more accurate classification. In the experiment, this paper uses SVM algorithm,\n",
    "AdaBoost algorithm and the algorithm of this paper to carry out several comparison experiments.\n",
    "\n",
    "Although AdaBoost is typically used to combine weak base learners (such as decision stumps),\n",
    "it has been shown that it can also effectively combine strong base learners\n",
    "(such as deep decision trees),  producing an even more accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3e8d1-af49-4c69-a339-8110d1e04b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c1281-13bf-4f7e-a4df-a4eda02133c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb0117-506d-4923-b0e3-a39e55ed6bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "The error function that AdaBoost uses is an exponential loss function.\n",
    "First we find the products between the true values of training samples and the overall prediction for each sample.\n",
    "Then we take the sum of all the exponentials of these products in order to compute the error at iteration m.\n",
    "\n",
    "The exponential loss is used in the AdaBoost algorithm. The principal attraction of exponential loss \n",
    "in the context of additive modeling is computational. The additive expansion produced by AdaBoost is\n",
    "estimating onehalf of the log-odds of P(Y = 1|x). \n",
    "This justifies using its sign as the classification rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dca478-b4fb-4d30-9068-2cc6fb4bb8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574db2c4-1ad7-45dd-9e4a-03b349ec9858",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ed5ecd-65ad-4d84-b3a2-db0a5ee25f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "This is done by making misclassified cases to be updated with increased weights after an iteration. \n",
    "Increased weights would make our learning algorithm pay higher attention to these observations in the next iteration.\n",
    "\n",
    "\n",
    "After training a classifier at any level, AdaBoost assigns weight to each training item.\n",
    "Misclassified item is assigned a higher weight so that it appears in the training subset of \n",
    "the next classifier with a higher probability.\n",
    "\n",
    "Simply put, the idea is to set weights to both classifiers and data points (samples)\n",
    "in a way that forces classifiers to concentrate on observations that are difficult to correctly classify .\n",
    "This process is done sequentially in that the two weights are adjusted at each step as iterations of the algorithm proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d81c4-cf7c-473e-b4a2-2f4dfe2f2fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78146a-ccd8-43a1-803d-a3f69e5abc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b376fb-e9b7-487d-9a62-35d46d215cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "An important hyperparameter for Adaboost is n_estimator.\n",
    "Often by changing the number of base models or weak learners we can adjust the accuracy of the model. \n",
    "The number of trees added to the model must be high for the model to work well, often hundreds, if not thousands.\n",
    "\n",
    "\n",
    "An AdaBoost [1] classifier is a meta-estimator that begins by fitting a classifier\n",
    "on the original dataset and then fits additional copies of the classifier on the same\n",
    "dataset but where the weights of incorrectly classified instances\n",
    "are adjusted such that subsequent classifiers focus more on difficult cases.\n",
    "\n",
    "Few important parameters of AdaBoost are :\n",
    "base_estimator: It is a weak learner used to train the model.\n",
    "n_estimators: Number of weak learners to train in each iteration.\n",
    "learning_rate: It contributes to the weights of weak learners. It uses 1 as a default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a449178-b279-40fe-bba8-e17d35b21e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "///////////////////////////////////////////////THE END//////////////////////////////////////////////"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
